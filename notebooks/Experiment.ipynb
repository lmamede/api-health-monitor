{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGCJ6Fn9PCKb"
   },
   "source": [
    "# Preparing the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c51Jo8NhO_Nx"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:58:48.104587100Z",
     "start_time": "2026-02-18T18:58:48.084371800Z"
    }
   },
   "source": [
    "#%pip install --upgrade pip\n",
    "#%pip install pandas\n",
    "#%pip install scipy \n",
    "#%pip install scikit-learn \n",
    "#%pip install tqdm \n",
    "#%pip install plotly \n",
    "#%pip install matplotlib\n",
    "#%pip install nbformat\n",
    "#%pip install fastparquet\n",
    "#%pip install pyarrow\n",
    "#%pip install seaborn"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 1379,
     "status": "ok",
     "timestamp": 1760974113149,
     "user": {
      "displayName": "Lorena Mamede",
      "userId": "00211236421092021130"
     },
     "user_tz": 180
    },
    "id": "BdL9lh4zO6yJ",
    "ExecuteTime": {
     "end_time": "2026-02-18T18:58:48.401644500Z",
     "start_time": "2026-02-18T18:58:48.105569800Z"
    }
   },
   "source": [
    "# requirements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2, poisson, chisquare\n",
    "from scipy.stats import entropy  # for KL (use small-smoothing)\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training phase: identifying typical traffic behavior"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:58:48.414788500Z",
     "start_time": "2026-02-18T18:58:48.403751300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrafficLearner:\n",
    "    def __init__(self, window_sizes, path_normal_traffic_df):\n",
    "        self.window_sizes = window_sizes\n",
    "        self.path_normal_traffic_df = path_normal_traffic_df\n",
    "        self.raw_normal_traffic_df = None\n",
    "\n",
    "    def get_normal_traffic_df(self):\n",
    "        if self.raw_normal_traffic_df is None:\n",
    "            self.raw_normal_traffic_df = pd.read_csv(self.path_normal_traffic_df, low_memory=False)\n",
    "            self.raw_normal_traffic_df['time_local'] = pd.to_datetime(self.raw_normal_traffic_df['time_local'])\n",
    "        return self.raw_normal_traffic_df\n",
    "\n",
    "    def index_windows(self, df):\n",
    "        windows_df = []\n",
    "\n",
    "        for window_size in self.window_sizes:\n",
    "            df['window_start'] = df['time_local'].dt.floor(window_size).copy()\n",
    "            df['window_id'] = (\n",
    "                    df['window_start']\n",
    "                    .astype(int)\n",
    "                    .rank(method='dense')\n",
    "                    .astype(int) - 1\n",
    "            )\n",
    "\n",
    "            df['window_size'] = int(window_size.replace('s', ''))\n",
    "\n",
    "            windows_df.append(\n",
    "                df[[\n",
    "                    'endpoint',\n",
    "                    'window_size',\n",
    "                    'window_id',\n",
    "                    'window_start',\n",
    "                    'time_local',\n",
    "                    'total_requests',\n",
    "                    'is_anomaly'\n",
    "                ]]\n",
    "            )\n",
    "\n",
    "        return pd.concat(windows_df, ignore_index=True)\n",
    "\n",
    "    def learn_traffic_information(self):\n",
    "        df = self.get_normal_traffic_df()\n",
    "        df = self.index_windows(df)\n",
    "\n",
    "        window_stats = (\n",
    "            df.groupby([\"window_size\", \"endpoint\", \"window_id\"])\n",
    "            .agg(lam=(\"total_requests\", \"mean\"))\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        return window_stats\n",
    "\n",
    "    def label_test_windows(self, df):\n",
    "        df = self.index_windows(df)\n",
    "\n",
    "        window_stats = (\n",
    "            df.groupby([\"window_size\",\"endpoint\", \"window_id\"])\n",
    "            .agg(has_anomaly=(\"is_anomaly\", \"max\"))\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        return window_stats\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring phase: observes traffic for atypical behavior"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:58:48.425155500Z",
     "start_time": "2026-02-18T18:58:48.415805600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Traffic Features Extractor\n",
    "def kl_divergence(p, q):\n",
    "    ''' Calculates KL divergence between p and q'''\n",
    "    SIG_EPS = 1e-10 # avoids division by zero\n",
    "    p = np.asarray(p, dtype=float) + SIG_EPS\n",
    "    q = np.asarray(q, dtype=float) + SIG_EPS\n",
    "    return entropy(p, q)\n",
    "\n",
    "def js_divergence(p, q, eps=1e-12):\n",
    "    p = np.asarray(p) + eps\n",
    "    q = np.asarray(q) + eps\n",
    "    p /= p.sum()\n",
    "    q /= q.sum()\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "def calculate_D(pmf_y, dX):\n",
    "    return js_divergence(pmf_y, dX)\n",
    "\n",
    "def calculate_Delta(Xbar, lambda_ep):\n",
    "    return (Xbar - lambda_ep) / max(lambda_ep, 1)\n",
    "\n",
    "def calculate_ZScore(Xbar, lambda_ep, seconds_in_window):\n",
    "    return (Xbar - lambda_ep) / math.sqrt(max(lambda_ep, 1) / seconds_in_window)\n",
    "\n",
    "def sample_expected_distribution(max_count, lambda_ep):\n",
    "    bins = np.arange(0, max_count+1)\n",
    "    dY = poisson.pmf(bins, mu=lambda_ep)\n",
    "    dY = dY / dY.sum()\n",
    "    return dY\n",
    "\n",
    "def get_observed_distribution(max_count, current_window):\n",
    "    obs_counts, _ = np.histogram(\n",
    "        current_window,\n",
    "        bins=np.arange(0, max_count+2)\n",
    "    )\n",
    "    dX = obs_counts / obs_counts.sum()\n",
    "    return dX\n",
    "\n",
    "def extract_traffic_changes(current_window, lambda_ep, seconds_in_window):\n",
    "    max_count = max(\n",
    "        int(current_window.max()),\n",
    "        int(lambda_ep*3)+5\n",
    "    )\n",
    "\n",
    "    Xbar = current_window.mean()\n",
    "    dX = get_observed_distribution(max_count, current_window)\n",
    "    dY = sample_expected_distribution(max_count, lambda_ep)\n",
    "\n",
    "    D = calculate_D(dX, dY)\n",
    "    #D_norm = D/np.log(2)\n",
    "    Delta = calculate_Delta(Xbar, lambda_ep)\n",
    "    Z = calculate_ZScore(Xbar, lambda_ep, seconds_in_window)\n",
    "    return D, Delta, Z\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:58:48.434179900Z",
     "start_time": "2026-02-18T18:58:48.425155500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fuzzy Score\n",
    "def gaussian_membership(u, mu=0.0, sigma=1.0):\n",
    "    return math.exp(-((u-mu)**2) / (2*(sigma**2)))\n",
    "\n",
    "def fuzzification(current_window, D, Delta, Z):\n",
    "    sigma_u = max(1.0, np.std(current_window))  # adaptive width\n",
    "    fD = gaussian_membership(D, mu=0.0, sigma=sigma_u)\n",
    "    fDelta = gaussian_membership(Delta, mu=0.0, sigma=sigma_u)\n",
    "    fZ = gaussian_membership(Z, mu=0.0, sigma=sigma_u)\n",
    "    return fD, fDelta, fZ\n",
    "\n",
    "def anomaly_score(fD, fDelta, fZ):\n",
    "    fDprime = 1 - fD\n",
    "    fDelprime = 1 - fDelta\n",
    "    fZprime = 1 - fZ\n",
    "    eta = fDelprime + fDprime + fZprime\n",
    "    return eta/3\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:58:48.442148100Z",
     "start_time": "2026-02-18T18:58:48.435179700Z"
    }
   },
   "source": [
    "### MONITORING MODULE: Observes the traffic and compare with the registered lambdas\n",
    "def analyze_window(current_window, lambda_endpoint, beta, seconds_in_window):\n",
    "    ''''''\n",
    "    D, Delta, Z = extract_traffic_changes(current_window, lambda_endpoint, seconds_in_window)\n",
    "    fD, fDelta, fZ = fuzzification(current_window, D, Delta, Z)\n",
    "    eta = anomaly_score(fD, fDelta, fZ)\n",
    "\n",
    "    C2= -math.tanh((eta - beta)*2)\n",
    "\n",
    "    return {\n",
    "        'D': D,\n",
    "        'Delta': Delta,\n",
    "        'Z': Z,\n",
    "        'eta': eta,\n",
    "        'C2': C2,\n",
    "        'fD': fD,\n",
    "        'fDelta': fDelta,\n",
    "        'fZ': fZ\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:58:48.453495100Z",
     "start_time": "2026-02-18T18:58:48.443147200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IndicatorCalculator:\n",
    "    def __init__(self, endpoint, initial_Ra=1.0, model=\"logistic\", params=None):\n",
    "        self.endpoint = endpoint\n",
    "        self.Ra = float(initial_Ra)\n",
    "        self.model = model\n",
    "        self.params = params or {}\n",
    "        self.P = self.params.get(\"P0\", 0.1)\n",
    "        self.history = []\n",
    "\n",
    "    def update(self, eta=None, C2=None):\n",
    "        if self.model == \"sigmoid\":\n",
    "            alpha = self.params.get(\"alpha\", 3.063)\n",
    "            beta = self.params.get(\"beta\", 0.5)\n",
    "            self.Ra = 1 / (1 + np.exp(-alpha * ((self.Ra + C2) - beta)))\n",
    "\n",
    "        elif self.model == \"logistic\":\n",
    "            gamma = self.params.get(\"gamma\", 0.2)\n",
    "            self.Ra += gamma * C2 * self.Ra * (1 - self.Ra)\n",
    "\n",
    "        elif self.model == \"exponential\":\n",
    "            k = self.params.get(\"k\", 0.5)\n",
    "            anomaly = max(0, eta - 0.5)\n",
    "            self.Ra *= np.exp(-k * anomaly)\n",
    "\n",
    "        elif self.model == \"recovery\":\n",
    "            gamma = self.params.get(\"gamma\", 0.02)\n",
    "            delta = self.params.get(\"delta\", 0.2)\n",
    "            beta = self.params.get(\"beta\", 0.2)\n",
    "\n",
    "            anomaly = max(0, eta - beta)\n",
    "\n",
    "            recovery = gamma * (1 - self.Ra)\n",
    "            damage = delta * anomaly * self.Ra\n",
    "            self.Ra += recovery - damage\n",
    "\n",
    "        elif self.model == \"kalman\":\n",
    "            self._update_kalman(eta)\n",
    "\n",
    "        self.Ra = np.clip(self.Ra, 0, 1)\n",
    "\n",
    "    def _update_kalman(self, eta):\n",
    "        # parameters\n",
    "        Q = self.params.get(\"Q\", 0.005)   # process noise\n",
    "        R = self.params.get(\"R\", 0.05)    # observation noise\n",
    "        k = self.params.get(\"k\", 3.0)     # eta sensitivity\n",
    "\n",
    "        # convert eta → health observation\n",
    "        z = np.exp(-k * eta)\n",
    "\n",
    "        # prediction step\n",
    "        Ra_pred = self.Ra\n",
    "        P_pred = self.P + Q\n",
    "\n",
    "        # Kalman gain\n",
    "        K = P_pred / (P_pred + R)\n",
    "\n",
    "        # correction step\n",
    "        self.Ra = Ra_pred + K * (z - Ra_pred)\n",
    "\n",
    "        # update uncertainty\n",
    "        self.P = (1 - K) * P_pred\n",
    "\n",
    "    def record(self, window_id, info):\n",
    "        row = {\n",
    "            \"endpoint\": self.endpoint,\n",
    "            \"window_id\": window_id,\n",
    "            \"Ra\": self.Ra,\n",
    "            \"model\": self.model\n",
    "        }\n",
    "\n",
    "        row.update(info)\n",
    "\n",
    "        self.history.append(row)\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:58:48.463870300Z",
     "start_time": "2026-02-18T18:58:48.454493300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_lambda_series(train_df):\n",
    "    return (\n",
    "        train_df\n",
    "        .sort_values(\"window_id\")\n",
    "        .groupby(\"endpoint\")[\"lam\"]\n",
    "        .expanding()\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def simulate_traffic_monitor(train_df, obs_df, train_obs_df, window_size, indicators_calculators, beta):\n",
    "    obs_by_window = {\n",
    "        wid: {\n",
    "            \"total_requests\":g[\"total_requests\"].values,\n",
    "            \"window_start\": g['window_start'].values[0],\n",
    "        }\n",
    "        for wid, g in obs_df.groupby(\"window_id\")\n",
    "    }\n",
    "\n",
    "    train_obs_by_window = {\n",
    "        wid: g[\"total_requests\"].values\n",
    "        for wid, g in train_obs_df.groupby(\"window_id\")\n",
    "    }\n",
    "\n",
    "    lambda_series = compute_lambda_series(train_df)\n",
    "    previous_windows = []\n",
    "    window_ids = sorted(obs_by_window.keys())\n",
    "\n",
    "    for window_id in window_ids:\n",
    "        previous_windows.append(window_id - 1)\n",
    "        if window_id == 0:\n",
    "            lam = lambda_series.iloc[0]\n",
    "        else:\n",
    "            lam = lambda_series.iloc[window_id-1]\n",
    "\n",
    "        observed = obs_by_window.get(window_id)\n",
    "\n",
    "        if observed is None:\n",
    "            continue\n",
    "\n",
    "        expected = train_obs_by_window.get(window_id - 1, [])\n",
    "\n",
    "        out = analyze_window(\n",
    "            current_window=observed[\"total_requests\"],\n",
    "            lambda_endpoint=lam,\n",
    "            beta=beta,\n",
    "            seconds_in_window=window_size\n",
    "        )\n",
    "\n",
    "        info = {\n",
    "            \"lambda\": lam,\n",
    "            \"eta\": out[\"eta\"],\n",
    "            \"C2\": out[\"C2\"],\n",
    "            \"fDp\": 1-out[\"fD\"],\n",
    "            \"fDeltap\": 1-out[\"fDelta\"],\n",
    "            \"fZp\": 1-out[\"fZ\"],\n",
    "            \"expected\": expected,\n",
    "            \"window_start\": observed[\"window_start\"]\n",
    "        }\n",
    "\n",
    "        for ind in indicators_calculators:\n",
    "            ind.update(\n",
    "                eta=out[\"eta\"],\n",
    "                C2=out[\"C2\"]\n",
    "            )\n",
    "\n",
    "            ind.record(window_id, info)\n",
    "    return indicators_calculators"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Experiment"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:58:48.473174600Z",
     "start_time": "2026-02-18T18:58:48.464869300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def execute_experiment(train_df, obs_df, train_obs_df, window_size, beta):\n",
    "    results = []\n",
    "\n",
    "    for endpoint in tqdm(train_df.endpoint.unique()):\n",
    "        params={\"beta\": beta}\n",
    "\n",
    "        indicators = [\n",
    "            IndicatorCalculator(endpoint, model=\"sigmoid\", params=params),\n",
    "            IndicatorCalculator(endpoint, model=\"exponential\", params=params),\n",
    "            IndicatorCalculator(endpoint, model=\"recovery\", params=params),\n",
    "            IndicatorCalculator(endpoint, model=\"kalman\", params=params),\n",
    "        ]\n",
    "\n",
    "        indicators_report = simulate_traffic_monitor(\n",
    "            train_df[train_df.endpoint == endpoint],\n",
    "            obs_df[obs_df.endpoint == endpoint],\n",
    "            train_obs_df[train_obs_df.endpoint == endpoint],\n",
    "            window_size,\n",
    "            indicators,\n",
    "            beta\n",
    "        )\n",
    "\n",
    "        for ir in indicators_report:\n",
    "            results.extend(ir.history)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def experiment(normal_windows_lambda_df, window_obs, window_normal, window_sizes, beta_var):\n",
    "    results = []\n",
    "\n",
    "    for beta in beta_var:\n",
    "        for window_size in window_sizes:\n",
    "            print(f\"Running experiment: window_size={window_size} beta={beta}\")\n",
    "            size = int(window_size.replace('s', ''))\n",
    "\n",
    "            df = execute_experiment(\n",
    "                normal_windows_lambda_df[normal_windows_lambda_df['window_size'] == size],\n",
    "                window_obs[window_obs['window_size'] == size],\n",
    "                window_normal[window_normal['window_size'] == size],\n",
    "                size,\n",
    "                beta\n",
    "            )\n",
    "\n",
    "            df[\"window_size\"] = size\n",
    "            df[\"beta\"] = beta\n",
    "            results.append(df)\n",
    "\n",
    "    return pd.concat(results, ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "id": "8nRoN7lDlarq"
   },
   "cell_type": "markdown",
   "source": "# Evaluation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:58:48.496031500Z",
     "start_time": "2026-02-18T18:58:48.473174600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_gt(results_history, window_gt, window_sizes):\n",
    "    \"\"\"\n",
    "        @description Add the corresponding ground truth for each endpoint-window,\n",
    "                     so it can be compared later to generate perfomance metrics\n",
    "    \"\"\"\n",
    "\n",
    "    labeled_data= []\n",
    "    for window_size in window_sizes:\n",
    "        size = int(window_size.replace('s', ''))\n",
    "        labels = window_gt[window_gt['window_size']==size][['endpoint', 'window_id', 'has_anomaly']]\n",
    "        result_df = results_history[results_history['window_size'] == size]\n",
    "        df = pd.merge(result_df, labels, on=['endpoint','window_id'], how='left')\n",
    "        labeled_data.append(df)\n",
    "    return pd.concat(labeled_data, ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Full Pipeline"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset path"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:58:48.511058300Z",
     "start_time": "2026-02-18T18:58:48.497021200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ton_iot_train_path = \"../datasets/treated_dataset/ton_treated_train.csv\"\n",
    "ton_iot_test_path = \"../datasets/treated_dataset/ton_treated_test.csv\""
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Experiment params"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:58:48.521743600Z",
     "start_time": "2026-02-18T18:58:48.513058100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "WINDOW_SIZES = ['30s', '40s', '50s']\n",
    "BETA_VAR=[0.0, 0.3, 0.5, 0.7, 1.0]"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:58:51.683519500Z",
     "start_time": "2026-02-18T18:58:48.522743200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tl = TrafficLearner(\n",
    "    window_sizes=WINDOW_SIZES,\n",
    "    path_normal_traffic_df=ton_iot_train_path,\n",
    ")\n",
    "\n",
    "normal_traffic_lambda_df = tl.learn_traffic_information()\n",
    "normal_traffic_wind_observations_df = tl.index_windows(tl.get_normal_traffic_df())"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Monitoring"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:58:55.111217400Z",
     "start_time": "2026-02-18T18:58:51.945346500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_ton_test = pd.read_csv(ton_iot_test_path, low_memory=False)\n",
    "df_ton_test['time_local'] = pd.to_datetime(df_ton_test['time_local'])\n",
    "anomalous_traffic_label_df = tl.label_test_windows(df_ton_test.copy())\n",
    "anomalous_traffic_win_observations_df = tl.index_windows(df_ton_test)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T19:18:28.173046300Z",
     "start_time": "2026-02-18T18:58:55.124241800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = experiment(normal_traffic_lambda_df, anomalous_traffic_win_observations_df, normal_traffic_wind_observations_df, window_sizes=WINDOW_SIZES, beta_var=BETA_VAR)\n",
    "\n",
    "final = add_gt(results, anomalous_traffic_label_df, WINDOW_SIZES)\n",
    "final_sigmoid = final[final['model'] == 'sigmoid'].copy()\n",
    "final_kalman = final[final['model'] == 'kalman'].copy()\n",
    "\n",
    "final_sigmoid.to_csv(\"../results/sigmoid_results.csv\", index=False)\n",
    "final_kalman.to_csv(\"../results/kalman_results.csv\", index=False)\n",
    "anomalous_traffic_win_observations_df.to_csv(\"../datasets/treated_dataset/anomalous_traffic_win_observations.csv\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=30s beta=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:55<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=40s beta=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:43<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=50s beta=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:35<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=30s beta=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:52<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=40s beta=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:40<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=50s beta=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:32<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=30s beta=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:20<00:00,  5.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=40s beta=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:11<00:00,  4.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=50s beta=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:09<00:00,  4.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=30s beta=0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:27<00:00,  5.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=40s beta=0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=50s beta=0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:11<00:00,  4.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=30s beta=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:33<00:00,  5.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=40s beta=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:06<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=50s beta=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:56<00:00,  3.55s/it]\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
