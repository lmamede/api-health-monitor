{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGCJ6Fn9PCKb"
   },
   "source": [
    "# Preparing the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c51Jo8NhO_Nx"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:57:38.294593500Z",
     "start_time": "2026-02-21T02:57:38.278395100Z"
    }
   },
   "source": [
    "#%pip install --upgrade pip\n",
    "#%pip install pandas\n",
    "#%pip install scipy \n",
    "#%pip install scikit-learn \n",
    "#%pip install tqdm \n",
    "#%pip install nbformat\n",
    "#%pip install pyarrow"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 1379,
     "status": "ok",
     "timestamp": 1760974113149,
     "user": {
      "displayName": "Lorena Mamede",
      "userId": "00211236421092021130"
     },
     "user_tz": 180
    },
    "id": "BdL9lh4zO6yJ",
    "ExecuteTime": {
     "end_time": "2026-02-21T02:57:38.324619100Z",
     "start_time": "2026-02-21T02:57:38.296067500Z"
    }
   },
   "source": [
    "# requirements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training phase: identifying typical traffic behavior"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:57:38.380069900Z",
     "start_time": "2026-02-21T02:57:38.335625700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrafficLearner:\n",
    "    \"\"\"\n",
    "    Learns traffic normal behavior\n",
    "    \"\"\"\n",
    "    def __init__(self, window_sizes, path_normal_traffic_df):\n",
    "        self.window_sizes = window_sizes\n",
    "        self.path_normal_traffic_df = path_normal_traffic_df\n",
    "        self.raw_normal_traffic_df = None\n",
    "\n",
    "    def get_normal_traffic_df(self):\n",
    "        \"\"\"\n",
    "        Loads, format timestamp and creates a cache for\n",
    "        normal traffic dataset\n",
    "        \"\"\"\n",
    "        if self.raw_normal_traffic_df is None:\n",
    "            self.raw_normal_traffic_df = pd.read_csv(self.path_normal_traffic_df, low_memory=False)\n",
    "            self.raw_normal_traffic_df['time_local'] = pd.to_datetime(self.raw_normal_traffic_df['time_local'])\n",
    "        return self.raw_normal_traffic_df\n",
    "\n",
    "    def index_windows(self, df):\n",
    "        \"\"\"\n",
    "        Computes window size to index request to each\n",
    "        corresponding window\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): traffic request frequency by second\n",
    "\n",
    "        Returns:\n",
    "            dataframe with window indexed dataset\n",
    "        \"\"\"\n",
    "        windows_df = []\n",
    "\n",
    "        for window_size in self.window_sizes:\n",
    "            df['window_start'] = df['time_local'].dt.floor(window_size).copy()\n",
    "            df['window_id'] = (\n",
    "                    df['window_start']\n",
    "                    .astype(int)\n",
    "                    .rank(method='dense')\n",
    "                    .astype(int) - 1\n",
    "            )\n",
    "\n",
    "            df['window_size'] = int(window_size.replace('s', ''))\n",
    "\n",
    "            windows_df.append(\n",
    "                df[[\n",
    "                    'endpoint',\n",
    "                    'window_size',\n",
    "                    'window_id',\n",
    "                    'window_start',\n",
    "                    'time_local',\n",
    "                    'total_requests',\n",
    "                    'is_anomaly'\n",
    "                ]]\n",
    "            )\n",
    "\n",
    "        return pd.concat(windows_df, ignore_index=True)\n",
    "\n",
    "    def learn_traffic_information(self):\n",
    "        \"\"\"\n",
    "        Estimates lambda using each endpoint\n",
    "        window mean request frequency\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: with computed lambda per window\n",
    "        \"\"\"\n",
    "        df = self.get_normal_traffic_df()\n",
    "        df = self.index_windows(df)\n",
    "\n",
    "        window_stats = (\n",
    "            df.groupby([\"window_size\", \"endpoint\", \"window_id\"])\n",
    "            .agg(lam=(\"total_requests\", \"mean\"))\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        return window_stats\n",
    "\n",
    "    def label_test_windows(self, df):\n",
    "        df = self.index_windows(df)\n",
    "\n",
    "        window_stats = (\n",
    "            df.groupby([\"window_size\",\"endpoint\", \"window_id\"])\n",
    "            .agg(has_anomaly=(\"is_anomaly\", \"max\"))\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        return window_stats\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring phase: observes traffic for atypical behavior"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:57:38.389917500Z",
     "start_time": "2026-02-21T02:57:38.381071200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrafficFeatureExtractor:\n",
    "    def kl_divergence(self, p, q):\n",
    "        ''' Calculates KL divergence between p and q'''\n",
    "        SIG_EPS = 1e-10 # avoids division by zero\n",
    "        p = np.asarray(p, dtype=float) + SIG_EPS\n",
    "        q = np.asarray(q, dtype=float) + SIG_EPS\n",
    "        return entropy(p, q)\n",
    "\n",
    "    def js_divergence(self, p, q, eps=1e-12):\n",
    "        p = np.asarray(p) + eps\n",
    "        q = np.asarray(q) + eps\n",
    "        p /= p.sum()\n",
    "        q /= q.sum()\n",
    "        m = 0.5 * (p + q)\n",
    "        return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "    def calculate_D(self, pmf_y, dX):\n",
    "        return self.js_divergence(pmf_y, dX)\n",
    "\n",
    "    def calculate_Delta(self, Xbar, lambda_ep):\n",
    "        return (Xbar - lambda_ep) / max(lambda_ep, 1)\n",
    "\n",
    "    def calculate_ZScore(self, Xbar, lambda_ep, seconds_in_window):\n",
    "        return (Xbar - lambda_ep) / math.sqrt(max(lambda_ep, 1) / seconds_in_window)\n",
    "\n",
    "    def sample_expected_distribution(self, max_count, lambda_ep):\n",
    "        bins = np.arange(0, max_count+1)\n",
    "        dY = poisson.pmf(bins, mu=lambda_ep)\n",
    "        dY = dY / dY.sum()\n",
    "        return dY\n",
    "\n",
    "    def get_observed_distribution(self, max_count, current_window):\n",
    "        obs_counts, _ = np.histogram(\n",
    "            current_window,\n",
    "            bins=np.arange(0, max_count+2)\n",
    "        )\n",
    "        dX = obs_counts / obs_counts.sum()\n",
    "        return dX\n",
    "\n",
    "    def extract_traffic_changes(self, current_window, lambda_ep, seconds_in_window):\n",
    "        max_count = max(\n",
    "            int(current_window.max()),\n",
    "            int(lambda_ep*3)+5\n",
    "        )\n",
    "\n",
    "        Xbar = current_window.mean()\n",
    "        dX = self.get_observed_distribution(max_count, current_window)\n",
    "        dY = self.sample_expected_distribution(max_count, lambda_ep)\n",
    "\n",
    "        D = self.calculate_D(dX, dY)\n",
    "        Delta = self.calculate_Delta(Xbar, lambda_ep)\n",
    "        Z = self.calculate_ZScore(Xbar, lambda_ep, seconds_in_window)\n",
    "        return D, Delta, Z\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:57:38.399058400Z",
     "start_time": "2026-02-21T02:57:38.390905600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EndpointAnomalySensor:\n",
    "    def __init__(self, endpoint, lam):\n",
    "        self.endpoint = endpoint\n",
    "        self.lam = lam\n",
    "        self.tfe = TrafficFeatureExtractor()\n",
    "\n",
    "    def gaussian_membership(self, u, mu=0.0, sigma=1.0):\n",
    "        \"\"\"\n",
    "        Calculates the feature gaussian membership for\n",
    "        traffic normality\n",
    "\n",
    "        Args:\n",
    "            u: traffic feature\n",
    "            mu: the normality reference for the feature\n",
    "            sigma: deviation from normality\n",
    "\n",
    "        Returns:\n",
    "            float: value in [0.,1.] to assign no\n",
    "                    anomaly (0.) or huge anomaly (1.)\n",
    "        \"\"\"\n",
    "        return math.exp(-((u-mu)**2) / (2*(sigma**2)))\n",
    "\n",
    "    def fuzzification(self, current_window, D, Delta, Z):\n",
    "        \"\"\"\n",
    "        Traffic features are fuzzed using gaussian membership\n",
    "\n",
    "        Args:\n",
    "            current_window (np.array): request frequency per second array\n",
    "            D (float): divergence between observed and expected traffic\n",
    "            Delta (float): area difference between observed and expected traffic\n",
    "            Z: z-score of mean deviation between observed and expected traffic\n",
    "        Returns:\n",
    "            (float, float, float): membership of each feature\n",
    "        \"\"\"\n",
    "        sigma_u = max(1.0, np.std(current_window))  # adaptive width\n",
    "        fD = self.gaussian_membership(D, mu=0.0, sigma=sigma_u)\n",
    "        fDelta = self.gaussian_membership(Delta, mu=0.0, sigma=sigma_u)\n",
    "        fZ = self.gaussian_membership(Z, mu=0.0, sigma=sigma_u)\n",
    "        return fD, fDelta, fZ\n",
    "\n",
    "    def anomaly_score(self,fD, fDelta, fZ):\n",
    "        \"\"\"\n",
    "        Calculates anomaly score for traffic observations\n",
    "\n",
    "        Args:\n",
    "            fD (float): traffic divergence normality membership\n",
    "            fDelta (float): traffic area difference normality membership\n",
    "            fZ (float): z-score of mean deviation normality membership\n",
    "        \"\"\"\n",
    "        fDprime = 1 - fD\n",
    "        fDelprime = 1 - fDelta\n",
    "        fZprime = 1 - fZ\n",
    "        eta = fDelprime + fDprime + fZprime\n",
    "        return eta/3\n",
    "\n",
    "    def calculate_eta(self, current_window, seconds_in_window):\n",
    "        \"\"\"\n",
    "        Calculates traffic anomaly score\n",
    "\n",
    "        Args:\n",
    "            current_window (np.array): request frequency per second array\n",
    "            seconds_in_window (int): window size\n",
    "        Returns:\n",
    "            dict: traffic feature and anomaly measurements\n",
    "        \"\"\"\n",
    "        D, Delta, Z = self.tfe.extract_traffic_changes(current_window, self.lam, seconds_in_window)\n",
    "        fD, fDelta, fZ = self.fuzzification(current_window, D, Delta, Z)\n",
    "        eta = self.anomaly_score(fD, fDelta, fZ)\n",
    "\n",
    "        return {\n",
    "            'eta': eta,\n",
    "            'D': D,\n",
    "            'Delta': Delta,\n",
    "            'Z': Z,\n",
    "            'fDp': 1-fD,\n",
    "            'fDeltap': 1-fDelta,\n",
    "            'fZp': 1-fZ\n",
    "        }\n",
    "\n",
    "    def calculate_C2(self, eta, beta=0.5):\n",
    "        \"\"\"\n",
    "        Calculates frequency anomaly component\n",
    "\n",
    "        Args:\n",
    "            eta (float): anomaly score captured by sensor\n",
    "            beta (float): anomaly thresholds to accept or reject penalization\n",
    "        Returns:\n",
    "            float: computed component\n",
    "        \"\"\"\n",
    "        C2= -math.tanh((eta - beta)*2)\n",
    "\n",
    "        return C2\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:57:38.408453800Z",
     "start_time": "2026-02-21T02:57:38.399058400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RaCalculator:\n",
    "    def __init__(self, endpoint, initial_Ra=1.0, model=\"logistic\", params=None):\n",
    "        self.endpoint = endpoint\n",
    "        self.Ra = float(initial_Ra)\n",
    "        self.model = model\n",
    "        self.params = params or {}\n",
    "        self.P = self.params.get(\"P0\", 0.1)\n",
    "        self.history = []\n",
    "\n",
    "    def update_ra(self, eta=None, C2=None):\n",
    "        if self.model == \"sigmoid\":\n",
    "            alpha = self.params.get(\"alpha\", 3.063)\n",
    "            beta = self.params.get(\"beta\", 0.5)\n",
    "            self.Ra = 1 / (1 + np.exp(-alpha * ((self.Ra + C2) - beta)))\n",
    "\n",
    "        elif self.model == \"logistic\":\n",
    "            gamma = self.params.get(\"gamma\", 0.2)\n",
    "            self.Ra += gamma * C2 * self.Ra * (1 - self.Ra)\n",
    "\n",
    "        elif self.model == \"exponential\":\n",
    "            k = self.params.get(\"k\", 0.5)\n",
    "            anomaly = max(0, eta - 0.5)\n",
    "            self.Ra *= np.exp(-k * anomaly)\n",
    "\n",
    "        elif self.model == \"recovery\":\n",
    "            gamma = self.params.get(\"gamma\", 0.02)\n",
    "            delta = self.params.get(\"delta\", 0.2)\n",
    "            beta = self.params.get(\"beta\", 0.2)\n",
    "\n",
    "            anomaly = max(0, eta - beta)\n",
    "\n",
    "            recovery = gamma * (1 - self.Ra)\n",
    "            damage = delta * anomaly * self.Ra\n",
    "            self.Ra += recovery - damage\n",
    "\n",
    "        elif self.model == \"kalman\":\n",
    "            self._update_kalman(eta)\n",
    "\n",
    "        self.Ra = np.clip(self.Ra, 0, 1)\n",
    "\n",
    "    def _update_kalman(self, eta):\n",
    "        # parameters\n",
    "        Q = self.params.get(\"Q\", 0.005)   # process noise\n",
    "        R = self.params.get(\"R\", 0.05)    # observation noise\n",
    "        k = self.params.get(\"k\", 3.0)     # eta sensitivity\n",
    "\n",
    "        # convert eta → health observation\n",
    "        z = np.exp(-k * eta)\n",
    "\n",
    "        # prediction step\n",
    "        Ra_pred = self.Ra\n",
    "        P_pred = self.P + Q\n",
    "\n",
    "        # Kalman gain\n",
    "        K = P_pred / (P_pred + R)\n",
    "\n",
    "        # correction step\n",
    "        self.Ra = Ra_pred + K * (z - Ra_pred)\n",
    "\n",
    "        # update uncertainty\n",
    "        self.P = (1 - K) * P_pred\n",
    "\n",
    "    def record(self, window_id, info):\n",
    "        row = {\n",
    "            \"endpoint\": self.endpoint,\n",
    "            \"window_id\": window_id,\n",
    "            \"Ra\": self.Ra,\n",
    "            \"model\": self.model\n",
    "        }\n",
    "\n",
    "        row.update(info)\n",
    "        self.history.append(row)\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Experiment"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:57:38.418882300Z",
     "start_time": "2026-02-21T02:57:38.408453800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_lambda_series(train_df):\n",
    "    return (\n",
    "        train_df\n",
    "        .sort_values(\"window_id\")\n",
    "        .groupby(\"endpoint\")[\"lam\"]\n",
    "        .expanding()\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "def compute_anomaly_score_series(lambda_series, obs_by_window, train_obs_by_window, window_size, eas):\n",
    "    window_ids = sorted(obs_by_window.keys())\n",
    "    eta_series = []\n",
    "\n",
    "    for window_id in window_ids:\n",
    "        observed = obs_by_window.get(window_id)\n",
    "        if observed is None:\n",
    "            continue\n",
    "\n",
    "        if window_size > 0:\n",
    "            eas.lam = lambda_series.iloc[window_id-1]\n",
    "\n",
    "        out = eas.calculate_eta(\n",
    "            current_window=observed[\"total_requests\"],\n",
    "            seconds_in_window=window_size\n",
    "        )\n",
    "\n",
    "        out[\"lambda\"] = eas.lam\n",
    "        out[\"C2\"] = eas.calculate_C2(out[\"eta\"])\n",
    "        out[\"expected\"] = train_obs_by_window.get(window_id - 1, [])\n",
    "        out[\"window_start\"] = observed[\"window_start\"]\n",
    "        out[\"window_id\"] = window_id\n",
    "        eta_series.append(out)\n",
    "\n",
    "    return pd.DataFrame(eta_series)\n",
    "\n",
    "def compute_ra_series(eta_series, indicators_calculators):\n",
    "    window_ids = sorted(eta_series.keys())\n",
    "\n",
    "    for window_id in window_ids:\n",
    "        for ind in indicators_calculators:\n",
    "            ind.update_ra(eta=eta_series[window_id]['eta'])\n",
    "            ind.record(window_id, eta_series[window_id])\n",
    "    return indicators_calculators\n",
    "\n",
    "def format_window_info(obs_df, train_obs_df):\n",
    "    obs_by_window = {\n",
    "        wid: {\n",
    "            \"total_requests\":g[\"total_requests\"].values,\n",
    "            \"window_start\": g['window_start'].values[0],\n",
    "        }\n",
    "        for wid, g in obs_df.groupby(\"window_id\")\n",
    "    }\n",
    "\n",
    "    train_obs_by_window = {\n",
    "        wid: g[\"total_requests\"].values\n",
    "        for wid, g in train_obs_df.groupby(\"window_id\")\n",
    "    }\n",
    "\n",
    "    return obs_by_window, train_obs_by_window\n",
    "\n",
    "def format_eta_info(df_eta):\n",
    "    eta_by_window = {\n",
    "        wid: {\n",
    "            \"eta\": g[\"eta\"].values[0] if len(g[\"eta\"].values) == 1 else np.nan,\n",
    "            \"C2\":g[\"C2\"].values[0] if len(g[\"C2\"].values) == 1 else np.nan,\n",
    "            \"window_start\": g[\"window_start\"].values[0] if len(g[\"window_start\"].values) == 1 else np.nan,\n",
    "            \"fDeltap\":g[\"fDeltap\"].values[0] if len(g[\"fDeltap\"].values) == 1 else np.nan,\n",
    "            \"fDp\":g[\"fDp\"].values[0] if len(g[\"fDp\"].values) == 1 else np.nan,\n",
    "            \"fZp\":g[\"fZp\"].values[0] if len(g[\"fZp\"].values) == 1 else np.nan\n",
    "        }\n",
    "        for wid, g in df_eta.groupby(\"window_id\")\n",
    "    }\n",
    "\n",
    "    return eta_by_window\n",
    "\n",
    "def experiment_eta(train_df, obs_df, train_obs_df, window_size):\n",
    "    results_anomaly_detection = []\n",
    "\n",
    "    for endpoint in tqdm(train_df.endpoint.unique()):\n",
    "        lambda_series = compute_lambda_series(train_df[train_df.endpoint == endpoint])\n",
    "\n",
    "        obs_by_window, train_obs_by_window = format_window_info(\n",
    "            obs_df[obs_df.endpoint == endpoint],\n",
    "            train_obs_df[train_obs_df.endpoint == endpoint]\n",
    "        )\n",
    "\n",
    "        eas = EndpointAnomalySensor(\n",
    "            endpoint=endpoint,\n",
    "            lam=lambda_series.iloc[0]\n",
    "        )\n",
    "\n",
    "        df_anomaly_score_series = compute_anomaly_score_series(\n",
    "            lambda_series,\n",
    "            obs_by_window,\n",
    "            train_obs_by_window,\n",
    "            window_size,\n",
    "            eas\n",
    "        )\n",
    "\n",
    "        df_anomaly_score_series[\"endpoint\"] = endpoint\n",
    "\n",
    "        results_anomaly_detection.append(df_anomaly_score_series)\n",
    "    return pd.concat(results_anomaly_detection, ignore_index=True)\n",
    "\n",
    "def experiment_ra(train_df, obs_df, train_obs_df, window_size, params):\n",
    "    results = []\n",
    "    for endpoint in tqdm(train_df.endpoint.unique()):\n",
    "        lambda_series = compute_lambda_series(train_df[train_df.endpoint == endpoint])\n",
    "\n",
    "        obs_by_window, train_obs_by_window = format_window_info(\n",
    "            obs_df[obs_df.endpoint == endpoint],\n",
    "            train_obs_df[train_obs_df.endpoint == endpoint]\n",
    "        )\n",
    "\n",
    "        ra_calculators = [\n",
    "            #RaCalculator(endpoint, model=\"sigmoid\", params=params),\n",
    "            #RaCalculator(endpoint, model=\"exponential\", params=params),\n",
    "            #RaCalculator(endpoint, model=\"recovery\", params=params),\n",
    "            RaCalculator(endpoint, model=\"kalman\", params=params),\n",
    "        ]\n",
    "\n",
    "        eas = EndpointAnomalySensor(\n",
    "            endpoint=endpoint,\n",
    "            lam=lambda_series.iloc[0]\n",
    "        )\n",
    "\n",
    "        df_anomaly_score_series = compute_anomaly_score_series(\n",
    "            lambda_series,\n",
    "            obs_by_window,\n",
    "            train_obs_by_window,\n",
    "            window_size,\n",
    "            eas\n",
    "        )\n",
    "\n",
    "        eta_series = format_eta_info(df_anomaly_score_series)\n",
    "        indicators_report = compute_ra_series(eta_series, ra_calculators)\n",
    "\n",
    "        for ir in indicators_report:\n",
    "            results.extend(ir.history)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "id": "8nRoN7lDlarq"
   },
   "cell_type": "markdown",
   "source": "# Evaluation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:57:38.430167500Z",
     "start_time": "2026-02-21T02:57:38.420882800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_gt(results_history, window_gt, window_sizes):\n",
    "    \"\"\"\n",
    "        @description Add the corresponding ground truth for each endpoint-window,\n",
    "                     so it can be compared later to generate perfomance metrics\n",
    "    \"\"\"\n",
    "\n",
    "    labeled_data= []\n",
    "    for window_size in window_sizes:\n",
    "        size = int(window_size.replace('s', ''))\n",
    "        labels = window_gt[window_gt['window_size']==size][['endpoint', 'window_id', 'has_anomaly']]\n",
    "        result_df = results_history[results_history['window_size'] == size]\n",
    "        df = pd.merge(result_df, labels, on=['endpoint','window_id'], how='left')\n",
    "        labeled_data.append(df)\n",
    "    return pd.concat(labeled_data, ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Full Pipeline"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset path"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:57:38.436676Z",
     "start_time": "2026-02-21T02:57:38.431181700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ton_iot_train_path = \"../0-datasets/treated_dataset/ton_treated_train.csv\"\n",
    "ton_iot_test_path = \"../0-datasets/treated_dataset/ton_treated_test.csv\""
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Experiment params"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:57:38.441813500Z",
     "start_time": "2026-02-21T02:57:38.437674400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "WINDOW_SIZES = ['30s', '40s', '50s']\n",
    "BETA_VAR=[0.0, 0.3, 0.5, 0.7, 1.0]\n",
    "k_VAR=[1., 2., 3., 4., 5.]\n",
    "R_VAR = [0.01,0.05,0.1,0.2]\n",
    "Q_VAR = [0.001,0.005,0.01,0.02]"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:57:39.778700Z",
     "start_time": "2026-02-21T02:57:38.442815400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tl = TrafficLearner(\n",
    "    window_sizes=WINDOW_SIZES,\n",
    "    path_normal_traffic_df=ton_iot_train_path,\n",
    ")\n",
    "\n",
    "normal_traffic_lambda_df = tl.learn_traffic_information()\n",
    "normal_traffic_wind_observations_df = tl.index_windows(tl.get_normal_traffic_df())"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:57:40.147416900Z",
     "start_time": "2026-02-21T02:57:39.847556900Z"
    }
   },
   "cell_type": "code",
   "source": "normal_traffic_wind_observations_df.groupby(['window_id', 'window_size']).count()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                       endpoint  window_start  time_local  total_requests  \\\n",
       "window_id window_size                                                       \n",
       "0         30                160           160         160             160   \n",
       "          40                160           160         160             160   \n",
       "          50                260           260         260             260   \n",
       "1         30                300           300         300             300   \n",
       "          40                400           400         400             400   \n",
       "...                         ...           ...         ...             ...   \n",
       "3087      30                300           300         300             300   \n",
       "3088      30                300           300         300             300   \n",
       "3089      30                300           300         300             300   \n",
       "3090      30                300           300         300             300   \n",
       "3091      30                 90            90          90              90   \n",
       "\n",
       "                       is_anomaly  \n",
       "window_id window_size              \n",
       "0         30                  160  \n",
       "          40                  160  \n",
       "          50                  260  \n",
       "1         30                  300  \n",
       "          40                  400  \n",
       "...                           ...  \n",
       "3087      30                  300  \n",
       "3088      30                  300  \n",
       "3089      30                  300  \n",
       "3090      30                  300  \n",
       "3091      30                   90  \n",
       "\n",
       "[7266 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>endpoint</th>\n",
       "      <th>window_start</th>\n",
       "      <th>time_local</th>\n",
       "      <th>total_requests</th>\n",
       "      <th>is_anomaly</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>window_id</th>\n",
       "      <th>window_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>30</th>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>260</td>\n",
       "      <td>260</td>\n",
       "      <td>260</td>\n",
       "      <td>260</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>30</th>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3087</th>\n",
       "      <th>30</th>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3088</th>\n",
       "      <th>30</th>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3089</th>\n",
       "      <th>30</th>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3090</th>\n",
       "      <th>30</th>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3091</th>\n",
       "      <th>30</th>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7266 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:57:40.309758300Z",
     "start_time": "2026-02-21T02:57:40.184517800Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Collecting observations"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:57:42.088042300Z",
     "start_time": "2026-02-21T02:57:40.416079900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_ton_test = pd.read_csv(ton_iot_test_path, low_memory=False)\n",
    "df_ton_test['time_local'] = pd.to_datetime(df_ton_test['time_local'])\n",
    "anomalous_traffic_label_df = tl.label_test_windows(df_ton_test.copy())\n",
    "anomalous_traffic_win_observations_df = tl.index_windows(df_ton_test)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:57:42.232253900Z",
     "start_time": "2026-02-21T02:57:42.175277700Z"
    }
   },
   "cell_type": "code",
   "source": "anomalous_traffic_win_observations_df.groupby('window_size')['window_id'].count()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "window_size\n",
       "30    927250\n",
       "40    927250\n",
       "50    927250\n",
       "Name: window_id, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Eta experiments"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:57:42.394810100Z",
     "start_time": "2026-02-21T02:57:42.301312900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def window_size_experiment_eta(normal_windows_lambda_df, window_obs, window_normal, window_sizes):\n",
    "    results_eta = []\n",
    "    for window_size in window_sizes:\n",
    "        print(f\"Running experiment: window_size={window_size}\")\n",
    "        size = int(window_size.replace('s', ''))\n",
    "\n",
    "        df_eta = experiment_eta(\n",
    "            normal_windows_lambda_df[normal_windows_lambda_df['window_size'] == size],\n",
    "            window_obs[window_obs['window_size'] == size],\n",
    "            window_normal[window_normal['window_size'] == size],\n",
    "            size,\n",
    "        )\n",
    "\n",
    "        df_eta[\"window_size\"] = size\n",
    "        results_eta.append(df_eta)\n",
    "\n",
    "    return pd.concat(results_eta, ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:58:43.602615100Z",
     "start_time": "2026-02-21T02:57:42.445583300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_eta_results = window_size_experiment_eta(normal_traffic_lambda_df, anomalous_traffic_win_observations_df, normal_traffic_wind_observations_df, window_sizes=WINDOW_SIZES)\n",
    "df_eta_results = add_gt(df_eta_results, anomalous_traffic_label_df, WINDOW_SIZES)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:24<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:20<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.56s/it]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ra experiments"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "k variation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T03:01:41.175427300Z",
     "start_time": "2026-02-21T02:58:43.625588200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "k_results=[]\n",
    "for k in k_VAR:\n",
    "    params={\n",
    "        \"beta\": 0.5,\n",
    "        \"k\": k,\n",
    "    }\n",
    "    print(f\"Running experiment: k={k}\")\n",
    "    results = experiment_ra(normal_traffic_lambda_df, anomalous_traffic_win_observations_df, normal_traffic_wind_observations_df, window_size=30, params=params)\n",
    "    results[\"k\"] = k\n",
    "    results[\"window_size\"] = 30\n",
    "    k_results.append(results)\n",
    "\n",
    "df_k_results = pd.concat(k_results, ignore_index=True)\n",
    "df_k_l_results = add_gt(df_k_results, anomalous_traffic_label_df, WINDOW_SIZES)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: k=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:37<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: k=2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:37<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: k=3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:31<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: k=4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:34<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: k=5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:36<00:00,  3.63s/it]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "q variation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T03:03:43.299297Z",
     "start_time": "2026-02-21T03:01:41.205403200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "q_results = []\n",
    "for q in Q_VAR:\n",
    "    params={\n",
    "        \"beta\": 0.5,\n",
    "        \"Q\": q,\n",
    "    }\n",
    "    print(f\"Running experiment: Q={q}\")\n",
    "    results = experiment_ra(normal_traffic_lambda_df, anomalous_traffic_win_observations_df, normal_traffic_wind_observations_df, window_size=30, params=params)\n",
    "    results[\"Q\"] = q\n",
    "    results[\"window_size\"] = 30\n",
    "    q_results.append(results)\n",
    "\n",
    "df_q_results = pd.concat(q_results, ignore_index=True)\n",
    "df_q_l_results = add_gt(df_q_results, anomalous_traffic_label_df, WINDOW_SIZES)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: Q=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:30<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: Q=0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:30<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: Q=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:30<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: Q=0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:30<00:00,  3.00s/it]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "R variation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T03:05:45.064367600Z",
     "start_time": "2026-02-21T03:03:43.340010700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "r_results = []\n",
    "for r in R_VAR:\n",
    "    params={\n",
    "        \"beta\": 0.5,\n",
    "        \"R\": r,\n",
    "    }\n",
    "    print(f\"Running experiment: R={r}\")\n",
    "    results = experiment_ra(normal_traffic_lambda_df, anomalous_traffic_win_observations_df, normal_traffic_wind_observations_df, window_size=30, params=params)\n",
    "    results[\"R\"] = r\n",
    "    results[\"window_size\"] = 30\n",
    "    r_results.append(results)\n",
    "\n",
    "df_r_results = pd.concat(r_results, ignore_index=True)\n",
    "df_r_l_results = add_gt(df_r_results, anomalous_traffic_label_df, WINDOW_SIZES)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: R=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:30<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: R=0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:30<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: R=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:30<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: R=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:29<00:00,  3.00s/it]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Window variation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T03:05:45.124495100Z",
     "start_time": "2026-02-21T03:05:45.115983900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def window_size_experiment_ra(normal_windows_lambda_df, window_obs, window_normal, window_sizes, params):\n",
    "    results_ra = []\n",
    "    for window_size in window_sizes:\n",
    "        print(f\"Running experiment: window_size={window_size}\")\n",
    "        size = int(window_size.replace('s', ''))\n",
    "\n",
    "        df_ra = experiment_ra(\n",
    "            normal_windows_lambda_df[normal_windows_lambda_df['window_size'] == size],\n",
    "            window_obs[window_obs['window_size'] == size],\n",
    "            window_normal[window_normal['window_size'] == size],\n",
    "            window_size=size,\n",
    "            params=params\n",
    "        )\n",
    "\n",
    "        df_ra[\"window_size\"] = size\n",
    "        results_ra.append(df_ra)\n",
    "\n",
    "    return pd.concat(results_ra, ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T03:06:51.631054200Z",
     "start_time": "2026-02-21T03:05:45.125492900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_ra_win_results = window_size_experiment_ra(normal_traffic_lambda_df, anomalous_traffic_win_observations_df, normal_traffic_wind_observations_df, window_sizes=WINDOW_SIZES, params={})\n",
    "df_ra_win_results = add_gt(df_ra_win_results, anomalous_traffic_label_df, WINDOW_SIZES)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:28<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.69s/it]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T03:07:08.300192500Z",
     "start_time": "2026-02-21T03:06:51.683988600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_eta_results.to_csv(\"outputs/kalman_eta_results.csv\", index=False)\n",
    "df_k_l_results.to_csv(\"outputs/kalman_k_results.csv\", index=False)\n",
    "df_q_l_results.to_csv(\"outputs/kalman_q_results.csv\", index=False)\n",
    "df_r_l_results.to_csv(\"outputs/kalman_r_results.csv\", index=False)\n",
    "df_ra_win_results.to_csv(\"outputs/kalman_ra_win_results.csv\", index=False)\n",
    "anomalous_traffic_win_observations_df.to_csv(\"../0-datasets/treated_dataset/anomalous_traffic_win_observations.csv\")"
   ],
   "outputs": [],
   "execution_count": 22
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
