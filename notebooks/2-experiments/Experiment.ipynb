{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGCJ6Fn9PCKb"
   },
   "source": [
    "# Preparing the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c51Jo8NhO_Nx"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:26:25.139381700Z",
     "start_time": "2026-02-20T01:26:25.122396400Z"
    }
   },
   "source": [
    "#%pip install --upgrade pip\n",
    "#%pip install pandas\n",
    "#%pip install scipy \n",
    "#%pip install scikit-learn \n",
    "#%pip install tqdm \n",
    "#%pip install nbformat\n",
    "#%pip install pyarrow"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 1379,
     "status": "ok",
     "timestamp": 1760974113149,
     "user": {
      "displayName": "Lorena Mamede",
      "userId": "00211236421092021130"
     },
     "user_tz": 180
    },
    "id": "BdL9lh4zO6yJ",
    "ExecuteTime": {
     "end_time": "2026-02-20T01:26:25.164483900Z",
     "start_time": "2026-02-20T01:26:25.140381900Z"
    }
   },
   "source": [
    "# requirements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training phase: identifying typical traffic behavior"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:26:25.219736700Z",
     "start_time": "2026-02-20T01:26:25.168484500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrafficLearner:\n",
    "    \"\"\"\n",
    "    Learns traffic normal behavior\n",
    "    \"\"\"\n",
    "    def __init__(self, window_sizes, path_normal_traffic_df):\n",
    "        self.window_sizes = window_sizes\n",
    "        self.path_normal_traffic_df = path_normal_traffic_df\n",
    "        self.raw_normal_traffic_df = None\n",
    "\n",
    "    def get_normal_traffic_df(self):\n",
    "        \"\"\"\n",
    "        Loads, format timestamp and creates a cache for\n",
    "        normal traffic dataset\n",
    "        \"\"\"\n",
    "        if self.raw_normal_traffic_df is None:\n",
    "            self.raw_normal_traffic_df = pd.read_csv(self.path_normal_traffic_df, low_memory=False)\n",
    "            self.raw_normal_traffic_df['time_local'] = pd.to_datetime(self.raw_normal_traffic_df['time_local'])\n",
    "        return self.raw_normal_traffic_df\n",
    "\n",
    "    def index_windows(self, df):\n",
    "        \"\"\"\n",
    "        Computes window size to index request to each\n",
    "        corresponding window\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): traffic request frequency by second\n",
    "\n",
    "        Returns:\n",
    "            dataframe with window indexed dataset\n",
    "        \"\"\"\n",
    "        windows_df = []\n",
    "\n",
    "        for window_size in self.window_sizes:\n",
    "            df['window_start'] = df['time_local'].dt.floor(window_size).copy()\n",
    "            df['window_id'] = (\n",
    "                    df['window_start']\n",
    "                    .astype(int)\n",
    "                    .rank(method='dense')\n",
    "                    .astype(int) - 1\n",
    "            )\n",
    "\n",
    "            df['window_size'] = int(window_size.replace('s', ''))\n",
    "\n",
    "            windows_df.append(\n",
    "                df[[\n",
    "                    'endpoint',\n",
    "                    'window_size',\n",
    "                    'window_id',\n",
    "                    'window_start',\n",
    "                    'time_local',\n",
    "                    'total_requests',\n",
    "                    'is_anomaly'\n",
    "                ]]\n",
    "            )\n",
    "\n",
    "        return pd.concat(windows_df, ignore_index=True)\n",
    "\n",
    "    def learn_traffic_information(self):\n",
    "        \"\"\"\n",
    "        Estimates lambda using each endpoint\n",
    "        window mean request frequency\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: with computed lambda per window\n",
    "        \"\"\"\n",
    "        df = self.get_normal_traffic_df()\n",
    "        df = self.index_windows(df)\n",
    "\n",
    "        window_stats = (\n",
    "            df.groupby([\"window_size\", \"endpoint\", \"window_id\"])\n",
    "            .agg(lam=(\"total_requests\", \"mean\"))\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        return window_stats\n",
    "\n",
    "    def label_test_windows(self, df):\n",
    "        df = self.index_windows(df)\n",
    "\n",
    "        window_stats = (\n",
    "            df.groupby([\"window_size\",\"endpoint\", \"window_id\"])\n",
    "            .agg(has_anomaly=(\"is_anomaly\", \"max\"))\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        return window_stats\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring phase: observes traffic for atypical behavior"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:26:25.253969900Z",
     "start_time": "2026-02-20T01:26:25.235746800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrafficFeatureExtractor:\n",
    "    def kl_divergence(self, p, q):\n",
    "        ''' Calculates KL divergence between p and q'''\n",
    "        SIG_EPS = 1e-10 # avoids division by zero\n",
    "        p = np.asarray(p, dtype=float) + SIG_EPS\n",
    "        q = np.asarray(q, dtype=float) + SIG_EPS\n",
    "        return entropy(p, q)\n",
    "\n",
    "    def js_divergence(self, p, q, eps=1e-12):\n",
    "        p = np.asarray(p) + eps\n",
    "        q = np.asarray(q) + eps\n",
    "        p /= p.sum()\n",
    "        q /= q.sum()\n",
    "        m = 0.5 * (p + q)\n",
    "        return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "    def calculate_D(self, pmf_y, dX):\n",
    "        return self.js_divergence(pmf_y, dX)\n",
    "\n",
    "    def calculate_Delta(self, Xbar, lambda_ep):\n",
    "        return (Xbar - lambda_ep) / max(lambda_ep, 1)\n",
    "\n",
    "    def calculate_ZScore(self, Xbar, lambda_ep, seconds_in_window):\n",
    "        return (Xbar - lambda_ep) / math.sqrt(max(lambda_ep, 1) / seconds_in_window)\n",
    "\n",
    "    def sample_expected_distribution(self, max_count, lambda_ep):\n",
    "        bins = np.arange(0, max_count+1)\n",
    "        dY = poisson.pmf(bins, mu=lambda_ep)\n",
    "        dY = dY / dY.sum()\n",
    "        return dY\n",
    "\n",
    "    def get_observed_distribution(self, max_count, current_window):\n",
    "        obs_counts, _ = np.histogram(\n",
    "            current_window,\n",
    "            bins=np.arange(0, max_count+2)\n",
    "        )\n",
    "        dX = obs_counts / obs_counts.sum()\n",
    "        return dX\n",
    "\n",
    "    def extract_traffic_changes(self, current_window, lambda_ep, seconds_in_window):\n",
    "        max_count = max(\n",
    "            int(current_window.max()),\n",
    "            int(lambda_ep*3)+5\n",
    "        )\n",
    "\n",
    "        Xbar = current_window.mean()\n",
    "        dX = self.get_observed_distribution(max_count, current_window)\n",
    "        dY = self.sample_expected_distribution(max_count, lambda_ep)\n",
    "\n",
    "        D = self.calculate_D(dX, dY)\n",
    "        Delta = self.calculate_Delta(Xbar, lambda_ep)\n",
    "        Z = self.calculate_ZScore(Xbar, lambda_ep, seconds_in_window)\n",
    "        return D, Delta, Z\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:26:25.269127900Z",
     "start_time": "2026-02-20T01:26:25.259039900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EndpointAnomalySensor:\n",
    "    def __init__(self, endpoint, lam):\n",
    "        self.endpoint = endpoint\n",
    "        self.lam = lam\n",
    "        self.tfe = TrafficFeatureExtractor()\n",
    "\n",
    "    def gaussian_membership(self, u, mu=0.0, sigma=1.0):\n",
    "        \"\"\"\n",
    "        Calculates the feature gaussian membership for\n",
    "        traffic normality\n",
    "\n",
    "        Args:\n",
    "            u: traffic feature\n",
    "            mu: the normality reference for the feature\n",
    "            sigma: deviation from normality\n",
    "\n",
    "        Returns:\n",
    "            float: value in [0.,1.] to assign no\n",
    "                    anomaly (0.) or huge anomaly (1.)\n",
    "        \"\"\"\n",
    "        return math.exp(-((u-mu)**2) / (2*(sigma**2)))\n",
    "\n",
    "    def fuzzification(self, current_window, D, Delta, Z):\n",
    "        \"\"\"\n",
    "        Traffic features are fuzzed using gaussian membership\n",
    "\n",
    "        Args:\n",
    "            current_window (np.array): request frequency per second array\n",
    "            D (float): divergence between observed and expected traffic\n",
    "            Delta (float): area difference between observed and expected traffic\n",
    "            Z: z-score of mean deviation between observed and expected traffic\n",
    "        Returns:\n",
    "            (float, float, float): membership of each feature\n",
    "        \"\"\"\n",
    "        sigma_u = max(1.0, np.std(current_window))  # adaptive width\n",
    "        fD = self.gaussian_membership(D, mu=0.0, sigma=sigma_u)\n",
    "        fDelta = self.gaussian_membership(Delta, mu=0.0, sigma=sigma_u)\n",
    "        fZ = self.gaussian_membership(Z, mu=0.0, sigma=sigma_u)\n",
    "        return fD, fDelta, fZ\n",
    "\n",
    "    def anomaly_score(self,fD, fDelta, fZ):\n",
    "        \"\"\"\n",
    "        Calculates anomaly score for traffic observations\n",
    "\n",
    "        Args:\n",
    "            fD (float): traffic divergence normality membership\n",
    "            fDelta (float): traffic area difference normality membership\n",
    "            fZ (float): z-score of mean deviation normality membership\n",
    "        \"\"\"\n",
    "        fDprime = 1 - fD\n",
    "        fDelprime = 1 - fDelta\n",
    "        fZprime = 1 - fZ\n",
    "        eta = fDelprime + fDprime + fZprime\n",
    "        return eta/3\n",
    "\n",
    "    def calculate_eta(self, current_window, seconds_in_window):\n",
    "        \"\"\"\n",
    "        Calculates traffic anomaly score\n",
    "\n",
    "        Args:\n",
    "            current_window (np.array): request frequency per second array\n",
    "            seconds_in_window (int): window size\n",
    "        Returns:\n",
    "            dict: traffic feature and anomaly measurements\n",
    "        \"\"\"\n",
    "        D, Delta, Z = self.tfe.extract_traffic_changes(current_window, self.lam, seconds_in_window)\n",
    "        fD, fDelta, fZ = self.fuzzification(current_window, D, Delta, Z)\n",
    "        eta = self.anomaly_score(fD, fDelta, fZ)\n",
    "\n",
    "        return {\n",
    "            'eta': eta,\n",
    "            'D': D,\n",
    "            'Delta': Delta,\n",
    "            'Z': Z,\n",
    "            'fDp': 1-fD,\n",
    "            'fDeltap': 1-fDelta,\n",
    "            'fZp': 1-fZ\n",
    "        }\n",
    "\n",
    "    def calculate_C2(self, eta, beta=0.5):\n",
    "        \"\"\"\n",
    "        Calculates frequency anomaly component\n",
    "\n",
    "        Args:\n",
    "            eta (float): anomaly score captured by sensor\n",
    "            beta (float): anomaly thresholds to accept or reject penalization\n",
    "        Returns:\n",
    "            float: computed component\n",
    "        \"\"\"\n",
    "        C2= -math.tanh((eta - beta)*2)\n",
    "\n",
    "        return C2\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:26:25.278966800Z",
     "start_time": "2026-02-20T01:26:25.270144300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RaCalculator:\n",
    "    def __init__(self, endpoint, initial_Ra=1.0, model=\"logistic\", params=None):\n",
    "        self.endpoint = endpoint\n",
    "        self.Ra = float(initial_Ra)\n",
    "        self.model = model\n",
    "        self.params = params or {}\n",
    "        self.P = self.params.get(\"P0\", 0.1)\n",
    "        self.history = []\n",
    "\n",
    "    def update_ra(self, eta=None, C2=None):\n",
    "        if self.model == \"sigmoid\":\n",
    "            alpha = self.params.get(\"alpha\", 3.063)\n",
    "            beta = self.params.get(\"beta\", 0.5)\n",
    "            self.Ra = 1 / (1 + np.exp(-alpha * ((self.Ra + C2) - beta)))\n",
    "\n",
    "        elif self.model == \"logistic\":\n",
    "            gamma = self.params.get(\"gamma\", 0.2)\n",
    "            self.Ra += gamma * C2 * self.Ra * (1 - self.Ra)\n",
    "\n",
    "        elif self.model == \"exponential\":\n",
    "            k = self.params.get(\"k\", 0.5)\n",
    "            anomaly = max(0, eta - 0.5)\n",
    "            self.Ra *= np.exp(-k * anomaly)\n",
    "\n",
    "        elif self.model == \"recovery\":\n",
    "            gamma = self.params.get(\"gamma\", 0.02)\n",
    "            delta = self.params.get(\"delta\", 0.2)\n",
    "            beta = self.params.get(\"beta\", 0.2)\n",
    "\n",
    "            anomaly = max(0, eta - beta)\n",
    "\n",
    "            recovery = gamma * (1 - self.Ra)\n",
    "            damage = delta * anomaly * self.Ra\n",
    "            self.Ra += recovery - damage\n",
    "\n",
    "        elif self.model == \"kalman\":\n",
    "            self._update_kalman(eta)\n",
    "\n",
    "        self.Ra = np.clip(self.Ra, 0, 1)\n",
    "\n",
    "    def _update_kalman(self, eta):\n",
    "        # parameters\n",
    "        Q = self.params.get(\"Q\", 0.005)   # process noise\n",
    "        R = self.params.get(\"R\", 0.05)    # observation noise\n",
    "        k = self.params.get(\"k\", 3.0)     # eta sensitivity\n",
    "\n",
    "        # convert eta → health observation\n",
    "        z = np.exp(-k * eta)\n",
    "\n",
    "        # prediction step\n",
    "        Ra_pred = self.Ra\n",
    "        P_pred = self.P + Q\n",
    "\n",
    "        # Kalman gain\n",
    "        K = P_pred / (P_pred + R)\n",
    "\n",
    "        # correction step\n",
    "        self.Ra = Ra_pred + K * (z - Ra_pred)\n",
    "\n",
    "        # update uncertainty\n",
    "        self.P = (1 - K) * P_pred\n",
    "\n",
    "    def record(self, window_id, info):\n",
    "        row = {\n",
    "            \"endpoint\": self.endpoint,\n",
    "            \"window_id\": window_id,\n",
    "            \"Ra\": self.Ra,\n",
    "            \"model\": self.model\n",
    "        }\n",
    "\n",
    "        row.update(info)\n",
    "        self.history.append(row)\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Experiment"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:26:25.290463400Z",
     "start_time": "2026-02-20T01:26:25.279969200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_lambda_series(train_df):\n",
    "    return (\n",
    "        train_df\n",
    "        .sort_values(\"window_id\")\n",
    "        .groupby(\"endpoint\")[\"lam\"]\n",
    "        .expanding()\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "def compute_anomaly_score_series(lambda_series, obs_by_window, train_obs_by_window, window_size, eas):\n",
    "    window_ids = sorted(obs_by_window.keys())\n",
    "    eta_series = []\n",
    "\n",
    "    for window_id in window_ids:\n",
    "        observed = obs_by_window.get(window_id)\n",
    "        if observed is None:\n",
    "            continue\n",
    "\n",
    "        if window_size > 0:\n",
    "            eas.lam = lambda_series.iloc[window_id-1]\n",
    "\n",
    "        out = eas.calculate_eta(\n",
    "            current_window=observed[\"total_requests\"],\n",
    "            seconds_in_window=window_size\n",
    "        )\n",
    "\n",
    "        out[\"lambda\"] = eas.lam\n",
    "        out[\"C2\"] = eas.calculate_C2(out[\"eta\"])\n",
    "        out[\"expected\"] = train_obs_by_window.get(window_id - 1, [])\n",
    "        out[\"window_start\"] = observed[\"window_start\"]\n",
    "        out[\"window_id\"] = window_id\n",
    "        eta_series.append(out)\n",
    "\n",
    "    return pd.DataFrame(eta_series)\n",
    "\n",
    "def compute_ra_series(eta_series, indicators_calculators):\n",
    "    window_ids = sorted(eta_series.keys())\n",
    "\n",
    "    for window_id in window_ids:\n",
    "        for ind in indicators_calculators:\n",
    "            ind.update_ra(eta=eta_series[window_id]['eta'])\n",
    "            ind.record(window_id, {\n",
    "                \"eta\": eta_series[window_id]['eta'],\n",
    "                \"window_start\": eta_series[window_id]['window_start'],\n",
    "            })\n",
    "    return indicators_calculators\n",
    "\n",
    "def format_window_info(obs_df, train_obs_df):\n",
    "    obs_by_window = {\n",
    "        wid: {\n",
    "            \"total_requests\":g[\"total_requests\"].values,\n",
    "            \"window_start\": g['window_start'].values[0],\n",
    "        }\n",
    "        for wid, g in obs_df.groupby(\"window_id\")\n",
    "    }\n",
    "\n",
    "    train_obs_by_window = {\n",
    "        wid: g[\"total_requests\"].values\n",
    "        for wid, g in train_obs_df.groupby(\"window_id\")\n",
    "    }\n",
    "\n",
    "    return obs_by_window, train_obs_by_window\n",
    "\n",
    "def format_eta_info(df_eta):\n",
    "    eta_by_window = {\n",
    "        wid: {\n",
    "            \"eta\": g[\"eta\"].values[0] if len(g[\"eta\"].values) == 1 else np.nan,\n",
    "            \"C2\":g[\"C2\"].values[0] if len(g[\"C2\"].values) == 1 else np.nan,\n",
    "            \"window_start\": g[\"window_start\"].values[0] if len(g[\"window_start\"].values) == 1 else np.nan\n",
    "        }\n",
    "        for wid, g in df_eta.groupby(\"window_id\")\n",
    "    }\n",
    "\n",
    "    return eta_by_window\n",
    "\n",
    "def experiment_eta(train_df, obs_df, train_obs_df, window_size):\n",
    "    results_anomaly_detection = []\n",
    "\n",
    "    for endpoint in tqdm(train_df.endpoint.unique()):\n",
    "        lambda_series = compute_lambda_series(train_df[train_df.endpoint == endpoint])\n",
    "\n",
    "        obs_by_window, train_obs_by_window = format_window_info(\n",
    "            obs_df[obs_df.endpoint == endpoint],\n",
    "            train_obs_df[train_obs_df.endpoint == endpoint]\n",
    "        )\n",
    "\n",
    "        eas = EndpointAnomalySensor(\n",
    "            endpoint=endpoint,\n",
    "            lam=lambda_series.iloc[0]\n",
    "        )\n",
    "\n",
    "        df_anomaly_score_series = compute_anomaly_score_series(\n",
    "            lambda_series,\n",
    "            obs_by_window,\n",
    "            train_obs_by_window,\n",
    "            window_size,\n",
    "            eas\n",
    "        )\n",
    "\n",
    "        df_anomaly_score_series[\"endpoint\"] = endpoint\n",
    "\n",
    "        results_anomaly_detection.append(df_anomaly_score_series)\n",
    "    return pd.concat(results_anomaly_detection, ignore_index=True)\n",
    "\n",
    "def experiment_ra(train_df, obs_df, train_obs_df, window_size, params):\n",
    "    results = []\n",
    "    for endpoint in tqdm(train_df.endpoint.unique()):\n",
    "        lambda_series = compute_lambda_series(train_df[train_df.endpoint == endpoint])\n",
    "\n",
    "        obs_by_window, train_obs_by_window = format_window_info(\n",
    "            obs_df[obs_df.endpoint == endpoint],\n",
    "            train_obs_df[train_obs_df.endpoint == endpoint]\n",
    "        )\n",
    "\n",
    "        ra_calculators = [\n",
    "            #RaCalculator(endpoint, model=\"sigmoid\", params=params),\n",
    "            #RaCalculator(endpoint, model=\"exponential\", params=params),\n",
    "            #RaCalculator(endpoint, model=\"recovery\", params=params),\n",
    "            RaCalculator(endpoint, model=\"kalman\", params=params),\n",
    "        ]\n",
    "\n",
    "        eas = EndpointAnomalySensor(\n",
    "            endpoint=endpoint,\n",
    "            lam=lambda_series.iloc[0]\n",
    "        )\n",
    "\n",
    "        df_anomaly_score_series = compute_anomaly_score_series(\n",
    "            lambda_series,\n",
    "            obs_by_window,\n",
    "            train_obs_by_window,\n",
    "            window_size,\n",
    "            eas\n",
    "        )\n",
    "\n",
    "        eta_series = format_eta_info(df_anomaly_score_series)\n",
    "        indicators_report = compute_ra_series(eta_series, ra_calculators)\n",
    "\n",
    "        for ir in indicators_report:\n",
    "            results.extend(ir.history)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "id": "8nRoN7lDlarq"
   },
   "cell_type": "markdown",
   "source": "# Evaluation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:26:25.304790Z",
     "start_time": "2026-02-20T01:26:25.291466600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_gt(results_history, window_gt, window_sizes):\n",
    "    \"\"\"\n",
    "        @description Add the corresponding ground truth for each endpoint-window,\n",
    "                     so it can be compared later to generate perfomance metrics\n",
    "    \"\"\"\n",
    "\n",
    "    labeled_data= []\n",
    "    for window_size in window_sizes:\n",
    "        size = int(window_size.replace('s', ''))\n",
    "        labels = window_gt[window_gt['window_size']==size][['endpoint', 'window_id', 'has_anomaly']]\n",
    "        result_df = results_history[results_history['window_size'] == size]\n",
    "        df = pd.merge(result_df, labels, on=['endpoint','window_id'], how='left')\n",
    "        labeled_data.append(df)\n",
    "    return pd.concat(labeled_data, ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Full Pipeline"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset path"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:26:25.312017700Z",
     "start_time": "2026-02-20T01:26:25.305781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ton_iot_train_path = \"../datasets/treated_dataset/ton_treated_train.csv\"\n",
    "ton_iot_test_path = \"../datasets/treated_dataset/ton_treated_test.csv\""
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Experiment params"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:26:25.318383Z",
     "start_time": "2026-02-20T01:26:25.313017100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "WINDOW_SIZES = ['30s', '40s', '50s']\n",
    "BETA_VAR=[0.0, 0.3, 0.5, 0.7, 1.0]\n",
    "k_VAR=[1., 2., 3., 4., 5.]\n",
    "R_VAR = [0.01,0.05,0.1,0.2]\n",
    "Q_VAR = [0.001,0.005,0.01,0.02]"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:26:31.559122500Z",
     "start_time": "2026-02-20T01:26:25.318383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tl = TrafficLearner(\n",
    "    window_sizes=WINDOW_SIZES,\n",
    "    path_normal_traffic_df=ton_iot_train_path,\n",
    ")\n",
    "\n",
    "normal_traffic_lambda_df = tl.learn_traffic_information()\n",
    "normal_traffic_wind_observations_df = tl.index_windows(tl.get_normal_traffic_df())"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Collecting observations"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:26:35.190463400Z",
     "start_time": "2026-02-20T01:26:31.635708200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_ton_test = pd.read_csv(ton_iot_test_path, low_memory=False)\n",
    "df_ton_test['time_local'] = pd.to_datetime(df_ton_test['time_local'])\n",
    "anomalous_traffic_label_df = tl.label_test_windows(df_ton_test.copy())\n",
    "anomalous_traffic_win_observations_df = tl.index_windows(df_ton_test)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Eta experiments"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:26:35.220493800Z",
     "start_time": "2026-02-20T01:26:35.209091600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def window_size_experiment_eta(normal_windows_lambda_df, window_obs, window_normal, window_sizes):\n",
    "    results_eta = []\n",
    "    for window_size in window_sizes:\n",
    "        print(f\"Running experiment: window_size={window_size}\")\n",
    "        size = int(window_size.replace('s', ''))\n",
    "\n",
    "        df_eta = experiment_eta(\n",
    "            normal_windows_lambda_df[normal_windows_lambda_df['window_size'] == size],\n",
    "            window_obs[window_obs['window_size'] == size],\n",
    "            window_normal[window_normal['window_size'] == size],\n",
    "            size,\n",
    "        )\n",
    "\n",
    "        df_eta[\"window_size\"] = size\n",
    "        results_eta.append(df_eta)\n",
    "\n",
    "    return pd.concat(results_eta, ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:28:51.067505500Z",
     "start_time": "2026-02-20T01:26:35.221492800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_eta_results = window_size_experiment_eta(normal_traffic_lambda_df, anomalous_traffic_win_observations_df, normal_traffic_wind_observations_df, window_sizes=WINDOW_SIZES)\n",
    "df_eta_results = add_gt(df_eta_results, anomalous_traffic_label_df, WINDOW_SIZES)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:51<00:00,  3.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:42<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:40<00:00,  2.55s/it]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ra experiments"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "k variation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:35:30.441314100Z",
     "start_time": "2026-02-20T01:28:51.082497500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "k_results=[]\n",
    "for k in k_VAR:\n",
    "    params={\n",
    "        \"beta\": 0.5,\n",
    "        \"k\": k,\n",
    "    }\n",
    "    print(f\"Running experiment: k={k}\")\n",
    "    results = experiment_ra(normal_traffic_lambda_df, anomalous_traffic_win_observations_df, normal_traffic_wind_observations_df, window_size=30, params=params)\n",
    "    results[\"k\"] = k\n",
    "    results[\"window_size\"] = 30\n",
    "    k_results.append(results)\n",
    "\n",
    "df_k_results = pd.concat(k_results, ignore_index=True)\n",
    "df_k_l_results = add_gt(df_k_results, anomalous_traffic_label_df, WINDOW_SIZES)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: k=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:21<00:00,  5.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: k=2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:20<00:00,  5.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: k=3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:19<00:00,  4.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: k=4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:18<00:00,  4.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: k=5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:18<00:00,  4.88s/it]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "q variation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:40:42.070619400Z",
     "start_time": "2026-02-20T01:35:30.483173400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "q_results = []\n",
    "for q in Q_VAR:\n",
    "    params={\n",
    "        \"beta\": 0.5,\n",
    "        \"Q\": q,\n",
    "    }\n",
    "    print(f\"Running experiment: Q={q}\")\n",
    "    results = experiment_ra(normal_traffic_lambda_df, anomalous_traffic_win_observations_df, normal_traffic_wind_observations_df, window_size=30, params=params)\n",
    "    results[\"Q\"] = q\n",
    "    results[\"window_size\"] = 30\n",
    "    q_results.append(results)\n",
    "\n",
    "df_q_results = pd.concat(q_results, ignore_index=True)\n",
    "df_q_l_results = add_gt(df_q_results, anomalous_traffic_label_df, WINDOW_SIZES)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: Q=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:16<00:00,  4.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: Q=0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:18<00:00,  4.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: Q=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:18<00:00,  4.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: Q=0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:18<00:00,  4.89s/it]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "R variation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:47:41.402586Z",
     "start_time": "2026-02-20T01:40:42.108558500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "r_results = []\n",
    "for r in R_VAR:\n",
    "    params={\n",
    "        \"beta\": 0.5,\n",
    "        \"R\": r,\n",
    "    }\n",
    "    print(f\"Running experiment: R={r}\")\n",
    "    results = experiment_ra(normal_traffic_lambda_df, anomalous_traffic_win_observations_df, normal_traffic_wind_observations_df, window_size=30, params=params)\n",
    "    results[\"R\"] = r\n",
    "    results[\"window_size\"] = 30\n",
    "    r_results.append(results)\n",
    "\n",
    "df_r_results = pd.concat(r_results, ignore_index=True)\n",
    "df_r_l_results = add_gt(df_r_results, anomalous_traffic_label_df, WINDOW_SIZES)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: R=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:18<00:00,  4.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: R=0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:48<00:00,  6.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: R=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:56<00:00,  7.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: R=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:55<00:00,  7.23s/it]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Window variation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:47:41.504794600Z",
     "start_time": "2026-02-20T01:47:41.488762900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def window_size_experiment_ra(normal_windows_lambda_df, window_obs, window_normal, window_sizes, params):\n",
    "    results_ra = []\n",
    "    for window_size in window_sizes:\n",
    "        print(f\"Running experiment: window_size={window_size}\")\n",
    "        size = int(window_size.replace('s', ''))\n",
    "\n",
    "        df_ra = experiment_ra(\n",
    "            normal_windows_lambda_df[normal_windows_lambda_df['window_size'] == size],\n",
    "            window_obs[window_obs['window_size'] == size],\n",
    "            window_normal[window_normal['window_size'] == size],\n",
    "            window_size=size,\n",
    "            params=params\n",
    "        )\n",
    "\n",
    "        df_ra[\"window_size\"] = size\n",
    "        results_ra.append(df_ra)\n",
    "\n",
    "    return pd.concat(results_ra, ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:51:59.607792Z",
     "start_time": "2026-02-20T01:47:41.507794900Z"
    }
   },
   "cell_type": "code",
   "source": "df_ra_win_results = window_size_experiment_ra(normal_traffic_lambda_df, anomalous_traffic_win_observations_df, normal_traffic_wind_observations_df, window_sizes=WINDOW_SIZES, params={})",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:49<00:00,  6.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:20<00:00,  5.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: window_size=50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:06<00:00,  4.18s/it]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:51:59.782315800Z",
     "start_time": "2026-02-20T01:51:59.698925600Z"
    }
   },
   "cell_type": "code",
   "source": "df_ra_win_results = add_gt(df_ra_win_results, anomalous_traffic_label_df, WINDOW_SIZES)",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:53:08.945794200Z",
     "start_time": "2026-02-20T01:51:59.784325500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_eta_results.to_csv(\"../results/kalman_eta_results.csv\", index=False)\n",
    "df_k_l_results.to_csv(\"../results/kalman_k_results.csv\", index=False)\n",
    "df_q_l_results.to_csv(\"../results/kalman_q_results.csv\", index=False)\n",
    "df_r_l_results.to_csv(\"../results/kalman_r_results.csv\", index=False)\n",
    "anomalous_traffic_win_observations_df.to_csv(\"../datasets/treated_dataset/anomalous_traffic_win_observations.csv\")"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T01:53:09.991062300Z",
     "start_time": "2026-02-20T01:53:09.027551400Z"
    }
   },
   "cell_type": "code",
   "source": "df_ra_win_results.to_csv(\"../results/kalman_ra_win_results.csv\", index=False)",
   "outputs": [],
   "execution_count": 22
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
